{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec Model parallely using BlazingText on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determing the training data loacation and model output location on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = 'medium-text'\n",
    "train_folder = 'train'\n",
    "model_folder = 'model'\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_folder)\n",
    "s3_output_location = 's3://{}/{}'.format(bucket, model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a container in the same region with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "print(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the training resource should be allocated to this training session. Here I chose 4 ml.c3.2xlarge instances for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=4, \n",
    "                                         train_instance_type='ml.c4.2xlarge',\n",
    "                                         train_volume_size = 5,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the hyper-parameters for the model and start running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"batch_skipgram\",\n",
    "                             epochs=5,\n",
    "                             min_count=5,\n",
    "                             sampling_threshold=0.0001,\n",
    "                             learning_rate=0.05,\n",
    "                             window_size=8,\n",
    "                             vector_dim=300,\n",
    "                             negative_samples=5,\n",
    "                             batch_size=17, #  = (2*window_size + 1) (Preferred. Used only if mode is batch_skipgram)\n",
    "                             evaluation=False,\n",
    "                             subwords=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-28 20:59:00 Starting - Starting the training job...\n",
      "2019-11-28 20:59:01 Starting - Launching requested ML instances......\n",
      "2019-11-28 21:00:07 Starting - Preparing the instances for training...\n",
      "2019-11-28 21:00:59 Downloading - Downloading input data...\n",
      "2019-11-28 21:01:15 Training - Downloading the training image.\u001b[33mArguments: train\u001b[0m\n",
      "\u001b[32mArguments: train\u001b[0m\n",
      "\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[32mFound 10.0.77.119 for host algo-1\u001b[0m\n",
      "\u001b[32mFound 10.0.85.218 for host algo-2\u001b[0m\n",
      "\u001b[32mFound 10.0.117.112 for host algo-3\u001b[0m\n",
      "\u001b[32mFound 10.0.76.183 for host algo-4\u001b[0m\n",
      "\u001b[33mFound 10.0.77.119 for host algo-1\u001b[0m\n",
      "\u001b[33mFound 10.0.85.218 for host algo-2\u001b[0m\n",
      "\u001b[33mFound 10.0.117.112 for host algo-3\u001b[0m\n",
      "\u001b[33mFound 10.0.76.183 for host algo-4\u001b[0m\n",
      "\u001b[34mFound 10.0.77.119 for host algo-1\u001b[0m\n",
      "\u001b[34mFound 10.0.85.218 for host algo-2\u001b[0m\n",
      "\u001b[34mFound 10.0.117.112 for host algo-3\u001b[0m\n",
      "\u001b[34mFound 10.0.76.183 for host algo-4\u001b[0m\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31mFound 10.0.77.119 for host algo-1\u001b[0m\n",
      "\u001b[31mFound 10.0.85.218 for host algo-2\u001b[0m\n",
      "\u001b[31mFound 10.0.117.112 for host algo-3\u001b[0m\n",
      "\u001b[31mFound 10.0.76.183 for host algo-4\u001b[0m\n",
      "\u001b[32m[11/28/2019 21:01:45 WARNING 139765903947584] Loggers have already been setup.\u001b[0m\n",
      "\u001b[32m[11/28/2019 21:01:45 WARNING 139765903947584] Loggers have already been setup.\u001b[0m\n",
      "\u001b[32m[11/28/2019 21:01:45 INFO 139765903947584] nvidia-smi took: 0.0252130031586 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[32m[11/28/2019 21:01:45 INFO 139765903947584] Running distributed CPU BlazingText training using batch_skipgram on 4 hosts.\u001b[0m\n",
      "\u001b[32m[11/28/2019 21:01:45 INFO 139765903947584] Number of hosts: 4, master IP address: 10.0.77.119, host IP address: 10.0.85.218.\u001b[0m\n",
      "\u001b[33m[11/28/2019 21:01:45 WARNING 139621139449664] Loggers have already been setup.\u001b[0m\n",
      "\u001b[33m[11/28/2019 21:01:45 WARNING 139621139449664] Loggers have already been setup.\u001b[0m\n",
      "\u001b[33m[11/28/2019 21:01:45 INFO 139621139449664] nvidia-smi took: 0.0252180099487 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[33m[11/28/2019 21:01:45 INFO 139621139449664] Running distributed CPU BlazingText training using batch_skipgram on 4 hosts.\u001b[0m\n",
      "\u001b[33m[11/28/2019 21:01:45 INFO 139621139449664] Number of hosts: 4, master IP address: 10.0.77.119, host IP address: 10.0.117.112.\u001b[0m\n",
      "\n",
      "2019-11-28 21:01:31 Training - Training image download completed. Training in progress.\u001b[31m[11/28/2019 21:01:46 WARNING 140021859592000] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[11/28/2019 21:01:46 WARNING 140021859592000] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[11/28/2019 21:01:46 INFO 140021859592000] nvidia-smi took: 0.0251669883728 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[11/28/2019 21:01:46 INFO 140021859592000] Running distributed CPU BlazingText training using batch_skipgram on 4 hosts.\u001b[0m\n",
      "\u001b[31m[11/28/2019 21:01:46 INFO 140021859592000] Number of hosts: 4, master IP address: 10.0.77.119, host IP address: 10.0.77.119.\u001b[0m\n",
      "\u001b[31m[11/28/2019 21:01:46 INFO 140021859592000] HTTP server started....\u001b[0m\n",
      "\u001b[31m[11/28/2019 21:01:46 INFO 140021859592000] Processing /opt/ml/input/data/train/medium_corpus.txt . File size: 347 MB\u001b[0m\n",
      "\u001b[31mWarning: Permanently added 'algo-3,10.0.117.112' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[31mWarning: Permanently added 'algo-4,10.0.76.183' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[31mWarning: Permanently added 'algo-2,10.0.85.218' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[31mprocessor name: algo-3, number of processors: 4, rank: 2\u001b[0m\n",
      "\u001b[31mprocessor name: algo-2, number of processors: 4, rank: 1\u001b[0m\n",
      "\u001b[31mprocessor name: algo-1, number of processors: 4, rank: 0\u001b[0m\n",
      "\u001b[31mprocessor name: algo-4, number of processors: 4, rank: 3\u001b[0m\n",
      "\u001b[34m[11/28/2019 21:01:46 WARNING 140107616204608] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[11/28/2019 21:01:46 WARNING 140107616204608] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[11/28/2019 21:01:46 INFO 140107616204608] nvidia-smi took: 0.02525806427 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[11/28/2019 21:01:46 INFO 140107616204608] Running distributed CPU BlazingText training using batch_skipgram on 4 hosts.\u001b[0m\n",
      "\u001b[34m[11/28/2019 21:01:46 INFO 140107616204608] Number of hosts: 4, master IP address: 10.0.77.119, host IP address: 10.0.76.183.\u001b[0m\n",
      "\u001b[31mRead 10M words\u001b[0m\n",
      "\u001b[31mRead 20M words\u001b[0m\n",
      "\u001b[31mRead 30M words\u001b[0m\n",
      "\u001b[31mRead 40M words\u001b[0m\n",
      "\u001b[31mRead 50M words\u001b[0m\n",
      "\u001b[31mRead 55M words\u001b[0m\n",
      "\u001b[31mNumber of words:  169037\u001b[0m\n",
      "\u001b[31mAlpha: 0.0489  Progress: 2.14%  Million Words/sec: 3.03\u001b[0m\n",
      "\u001b[31mAlpha: 0.0464  Progress: 7.21%  Million Words/sec: 3.13\u001b[0m\n",
      "\u001b[31mAlpha: 0.0438  Progress: 12.31%  Million Words/sec: 3.07\u001b[0m\n",
      "\u001b[31mAlpha: 0.0412  Progress: 17.41%  Million Words/sec: 3.06\u001b[0m\n",
      "\u001b[31mAlpha: 0.0386  Progress: 22.49%  Million Words/sec: 2.91\u001b[0m\n",
      "\u001b[31mAlpha: 0.0361  Progress: 27.59%  Million Words/sec: 2.93\u001b[0m\n",
      "\u001b[31mAlpha: 0.0335  Progress: 32.70%  Million Words/sec: 2.97\u001b[0m\n",
      "\u001b[31mAlpha: 0.0309  Progress: 37.80%  Million Words/sec: 2.87\u001b[0m\n",
      "\u001b[31mAlpha: 0.0284  Progress: 42.88%  Million Words/sec: 2.90\u001b[0m\n",
      "\u001b[31mAlpha: 0.0258  Progress: 47.96%  Million Words/sec: 2.91\u001b[0m\n",
      "\u001b[31mAlpha: 0.0232  Progress: 53.06%  Million Words/sec: 2.93\u001b[0m\n",
      "\u001b[31mAlpha: 0.0207  Progress: 58.17%  Million Words/sec: 2.90\u001b[0m\n",
      "\u001b[31mAlpha: 0.0181  Progress: 63.28%  Million Words/sec: 2.90\u001b[0m\n",
      "\u001b[31mAlpha: 0.0155  Progress: 68.39%  Million Words/sec: 2.92\u001b[0m\n",
      "\u001b[31mAlpha: 0.0130  Progress: 73.45%  Million Words/sec: 2.87\u001b[0m\n",
      "\u001b[31mAlpha: 0.0104  Progress: 78.56%  Million Words/sec: 2.89\u001b[0m\n",
      "\u001b[31mAlpha: 0.0079  Progress: 83.65%  Million Words/sec: 2.89\u001b[0m\n",
      "\u001b[31mAlpha: 0.0053  Progress: 88.75%  Million Words/sec: 2.91\u001b[0m\n",
      "\u001b[31mAlpha: 0.0027  Progress: 93.83%  Million Words/sec: 2.89\u001b[0m\n",
      "\u001b[31mAlpha: 0.0000  Progress: 98.88%  Million Words/sec: 2.85\u001b[0m\n",
      "\u001b[31mAlpha: 0.0000  Progress: 100.00%  Million Words/sec: 2.77\n",
      "\u001b[0m\n",
      "\u001b[31mTraining finished!\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 2.77\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 100.77\u001b[0m\n",
      "\u001b[34m[11/28/2019 21:04:01 INFO 140107616204608] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\u001b[32m[11/28/2019 21:04:50 INFO 139765903947584] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\n",
      "2019-11-28 21:05:25 Uploading - Uploading generated training model\u001b[33m[11/28/2019 21:05:22 INFO 139621139449664] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\n",
      "2019-11-28 21:07:35 Completed - Training job completed\n",
      "Training seconds: 1584\n",
      "Billable seconds: 1584\n",
      "Processing time 530.234522819519 seconds\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "#start time\n",
    "bt_model.fit(inputs=data_channels, logs=True)\n",
    "\n",
    "#end time\n",
    "print('Processing time {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
